### Mem2Seq论文阅读笔记

《Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End
Task-Oriented Dialog Systems》

1, 论文的贡献：

Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories.（Mem2Seq是第一个将 MN的multi-hop attention与pointer network相结合的neural generative model）

2，核心算法

2.1 整体描述

![image-20201229172954461](C:\Users\xmh\AppData\Roaming\Typora\typora-user-images\image-20201229172954461.png)

Mem2seq由MemNN encoder和memory decoder组成。The MemNN encoder creates a vector representation of the dialog history. Then the memory decoder reads and copies the memory to generate a response.（MemNN encoder对对话历史进行编码，memory decoder读出memory,从中copy知识来生成回答）

对话历史$X = \{x_1,...,x_n,\$\}$,\$作为一个哨兵。KB中的知识元组$B = \{b_1,...,b_l\}$。外部知识由KB和对话历史组成,$U = [B;X]$,期望的回答$Y = \{y-1,...,y_m\}$,指针的期望输出为

$$ptr_i =\left\{ \begin{array}{rcl} max(z)       &      & {if \exist z s.t. y_i = u_z}\\ n+l+1    &      & {otherwise}\\  \end{array} \right. $$

where $u_z$ ∈ U is the input sequence and n + l + 1 is the sentinel position index.

2.2 Memory Encoder 

Mem2Seq uses a standard MemNN with adjacent weighted tying (Sukhbaatar et al., 2015) as an encoder. 输入U，使用可学习的嵌入矩阵$C = \{C^1 ,..., C^{K+1}\}$将U中tokens转化为向量。query vector $q^k$作为a reading head。

The model loops over K hops and it computes the attention weights at hop k for each
memory i using:

$p^k_{i} = Softmax((q^k)^TC^k_i)$

where $C^k_i = C^k(xi)$ is the memory content in position i.$p_k$ is a soft memory selector that decides the memory relevance with respect to the query vector $q_k$.($p_k$是一个软memory选择器，表示memory的内容与query的相关程度)，the model reads out the memory $o^k$ by the weighted sum over $C_{k+1}$.

$o^k = \underset{i}{\sum}{p^k_i C^{k+1}_i}$

query向量的更新方式：$q^{k+1} = q^{k} + o^{k}$



这部分和End-To-End Memory Networks的input和output表示类似，只是后者输入和输出使用了不同的嵌入矩阵。

2.3 Memory Decoder

decoder使用了MemNN和RNN。MemNN存储U（对话历史和KB）。GRU输出的隐含状态作为query向量。At each decoding step t, the GRU gets the previously generated word and the previous query as input,and it generates the new query vector. (上一时刻的输出词和上一个查询向量)

$h_t = GRU(C^1(\hat y_{t-1}), h_{t-1})$

the query $h_t$ is passed to the MemNN which will produce the token, where $h_0 $is the encoder vector $o^K$($h_t$作为查询向量，从MemNN中读取内容，就是上面的encoder部分)

在每个hop，会生成两个分布，one over all the words in the vocabulary ($P_{vocab}$), and one over the memory contents($P_{ptr}$), which are the dialog history and KB information.（一个是词汇表的注意力分布，一个是MN中的注意力分布)

$P_{vocab}$ is generated by concatenating the first hop attention read out and the current query vector.

$P_{vocab}(\hat y^t) = Softmax(W_1[h_t;O^1])$

$P_{ptr}$是MemNN的最后一跳的注意力权重。（encoder的最后一条的注意力分布p)

Our decoder generates tokens by pointing to the input words in the memory, which is a similar mechanism to the attention used in pointer networks.

2.3.1 Sentinel

Once the sentinel is chosen, our model generates the token from $P_{vocab}$, otherwise, it takes the memory con-tent using the $P_{ptr}$ distribution. （特殊的哨兵符号\$作为使用$P_{vocab}$分布还是 $P_{ptr}$分布的分界点。

2.3 Memory Content（内容存储形式）

- 对话历史：We add **temporal information** and **speaker information** in each token of X to capture the sequential dependencies. For example, “hello t1 $u” means “hello” at time step 1 spoken by a user.
- KB：使用(subject, relation, object)表示知识.During decoding stage, the object part is used as the generated word for $P_{ptr}$.($P_{ptr}$指向MN中的内容，然后copy出指向内容作为response) For instance, when the KB tuple (The Westin, Distance, 5 miles) is pointed,our model copies “5 miles” as an output word.



 