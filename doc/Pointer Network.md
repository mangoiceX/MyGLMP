#### Attention

注意力机制的启发：

人脑会对周围大量信息进行筛选，只对其中一小部分的信息进行重点处理，称为“注意力”。

注意力机制一般分为两种：

1. 自上而下的有意识的注意力，称为聚焦式注意力。聚焦式注意力是指由预定目的、依赖任务，主动有意识地聚焦于某一对象的注意力。
2. 自下而上的无意识的注意力，称为基于显著性的注意力。基于显著性的注意力是由外界驱动，无需主动干预，和任务无关。

在信息过载时，通过注意力机制对信息进行筛选，有针对的训练可以提高模型学习效率。常见的attention机制是聚焦式注意力机制。常见的形式是：对于输入$X = [x_1,...,x_N]\in R^{D*N}$,从N个输入中选出与当前任务最相关的输入。当前任务会用一个查询向量q表示其大概内容，然后通过q与X进行注意力运算得到注意力分布。

给定一个和任务相关的查询向量q，使用注意力变量$z\in[1,N]$来表示选择信息的索引位置。计算在给定q和X下，选择第i个输入向量的概率$\alpha_n$,

$$\alpha_n = p(z=n|X,q) = softmax(s(x_n,q)) = \frac{exp(s(x_n,q))}{\overset{N}{\underset{j=1}{\sum}}exp(s(x_n,q))} $$

其中$\alpha_n$称为注意力分布。

通过注意力分布和输入得到我们从输入中筛选出的注意力信息，分为软性注意力和硬性注意力。

- 软性注意力：根据注意力分布计算输入向量的期望，$att(X,q) = \overset{N}\sum{\alpha_i * X_i}$
- 硬性注意力：选择注意力分布中概率最大的值对应的输入向量。

##### Pointer Network

指针网络与上述attention机制的输出不同，它输出的是输入向量的下标序列。

根据论文《Pointer Networks》，传统的seq2seq模型是无法解决输出序列的词汇表会随着输入序列长度的改变而改变的问题的。（不知道为什么，没看懂）

设P为输入序列，C为输出序列。

$p(c^p|p;\theta) =\overset{m(p)}{\underset{i=1}{\prod}}{p_{\theta}(C_i|C_1,...,C_{i-1},p;\theta)}$                                                                （1）

传统的seq2seq模型使用一个RNN encode 输入向量$P_i$，另外一个RNN decode出$C_i$,至于中间过程，暂时不清楚。如果使用极大似然法，那么解空间过大，搜索代价高。而使用attention机制建模，可以将注意力分布作为Equation 1的近似解，加速了求解过程。

至于为什么attention可以解决seq2seq存在的问题？

seq2seq是对根据输入词汇表计算输出序列，当输入序列变化时，他无法预测不在输入词汇表中的内容；作者提出的Pointer Networks，它预测的时候每一步都找当前输入序列中权重最大的那个元素，而由于输出序列完全来自输入序列，它可以适应输入序列的长度变化。

##### 记忆增强网络

由于神经元本身无法容纳太多信息，所以可以参考计算机的设计，将记忆单元独立出来，在记忆单元和神经网络之间提供读写接口。

![image-20201223171305105](C:\Users\xmh\AppData\Roaming\Typora\typora-user-images\image-20201223171305105.png)

外部记忆单元M：可用一组向量组织信息，不参与运算。

主网络C：负责信息处理，与外界交互，同时通过读写操作和外部记忆单元交互。

读取模块R：根据主网络C生成的查询向量$q_r$，从外部记忆单元中读取相应信息,$r = R(M,q_r)$

写入模块W：根据主网络C生成的查询向量$q_w$和要写入的信息$\alpha$来更新外部记忆，$M = W（M,q_w,\alpha)$

下面是GLMP的模型结构：

![image-20201223172039179](C:\Users\xmh\AppData\Roaming\Typora\typora-user-images\image-20201223172039179.png)

Global Memory Encoder相当于主网络，输入是预处理的对话数据，输出是context RNN的输出和最后隐含态。其中最后隐含态就是主网络生成的查询向量，RNN的输出直接写入到了EK中。

